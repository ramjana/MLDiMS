#model config for base transformer (llama2-7b)
mode_name: "llama2-7b"
model_dim: 4096
num_heads: 32
num_kv_heads: 32
hidden_dim: 16384 
num_layers: 32
attn_dim: 64 
mlp_activations: ["silu","linear"]
enable_dropout: False
logits_via_embedding: False
normalization_epsilon: 1.0e-5
dtype: "float16"
weight_dtype: "float16"
attention_bias: False
dropout_rate: 0.0
qkv_bias: False
multiple: 256 


#use HF weights  or random init state
parameters_path: ""

activation_dtype: "float16"

scan_layers: False
param_scan_axis: 1

fused_qkv: True
fused_mlp: False

output_dir: ""



hardware: "cpu"

mesh_axes: ['data', 'fsdp','sequence', 'tensor', 'append']
logical_axis_rules: [
                       ['act_batch', ['data','fsdp']],
                       ['act_heads', ['tensor','sequence']],
                       ['act_kv_heads', ['tensor','sequence']],
                       ['act_embed_dim',['tensor']],
                       ['act_seqlen',['sequence']],
                       ['act_mlp',['tensor']],
                       ['act_kv',['tensor']],
                       ['act_kv_batch',['data','fsdp']],
                       ['act_kv_headdim',['tensor']],
                       ['act_vocab',['tensor','sequence']],
                       ['mlp',['tensor','append']],
                       ['vocab',['tensor','append']],
                       ['embed',['fsdp','append']],
                       ['norm',['tensor']],
                       ['heads',['tensor','append']],
                       ['kv',[]],
                       ['qkv',[]],
                       ['kv_heads',['tensor','append']],
                       ['kv_head_dim',[]],
                       ['kvcache_batch',[]],
                       ['kvcache_heads',['append','tensor']],
                       ['kvcache_kv',[]],
                       ['kvcache_seqlen',[]],
                    ]

data_sharding: [['data','fsdp','tensor', 'append']]

data_parallelism: 2
fsdp_parallelism: 1
tensor_parallelism: 4
append_parallelism: 1
sequence_parallelism: 1
pipeline_parallelism: 1
num_devices: 8
num_slices: 1

vocab_size: 32000  #powers of 2 sharding
tokenizer_path: "tokenizers/tokenizer_llama3.tiktoken"
add_bos: True
add_eos: True

tokenize_eval_data: True

#Training parameter
steps: 10
learning_rate: 3.e-5
cosine_learning_rate_final_fraction: 0.1
warmup_steps: 0.1
learning_rate_schedule_steps: -1
micro_batch_size_to_train_on: 4 
device_batch_size: 32 

log_period: 100 

learning_rate: 3.e-5
learning_rate_schedule_steps: -1

max_seq_length: 2048
max_prefill_predict_len: 64  #maximum  length for the prefill for append phase

#Rope parameters
rope_min_timescale: 1
rope_max_timescale: 10000


#FIXME remove them
data_axis_name : "data"
model_axis_name: "model"
model_axis_size: 4
fsdp: True
fsdp_modules : ["Transformer",]
fsdp_axis_name: "data"
fsdp_min_weight_size: 512

seed: 1234
max_batch_size: 32

#inference
inference_prefill_lengths: "256,512,1024,2048"
inference_stages: "prefill,append"
inference_loop_iters: 10
compute_dim_order: "0,1,2,3"
prompt: "i love to"
